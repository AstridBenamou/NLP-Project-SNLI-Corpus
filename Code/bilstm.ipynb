{"cells":[{"cell_type":"code","source":["!pip install -U torch==1.8.0 torchtext==0.9.0\n","\n","# Reload environment\n","exit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKIbZ9aS5nAe","executionInfo":{"status":"ok","timestamp":1651497377046,"user_tz":-120,"elapsed":106750,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}},"outputId":"857f5048-bef5-4bb3-c34d-d762944178f0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.8.0\n","  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n","\u001b[K     |████████████████████████████████| 735.5 MB 13 kB/s \n","\u001b[?25hCollecting torchtext==0.9.0\n","  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n","\u001b[K     |████████████████████████████████| 7.1 MB 45.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.12.0\n","    Uninstalling torchtext-0.12.0:\n","      Successfully uninstalled torchtext-0.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.8.0 torchtext-0.9.0\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BXpEJzdA5j3y","executionInfo":{"status":"ok","timestamp":1651497385493,"user_tz":-120,"elapsed":407,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}}},"outputs":[],"source":["import numpy\n","import torch\n","import torch.nn as nn\n","import pdb\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import time\n","from torch.autograd import Variable"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dEKAxAY95j31","executionInfo":{"status":"ok","timestamp":1651497385732,"user_tz":-120,"elapsed":245,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}}},"outputs":[],"source":["import sys\n","import torchtext\n","from torchtext.legacy import data\n","from torchtext.legacy import datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5vaNS6QK5j32","executionInfo":{"status":"ok","timestamp":1651497389457,"user_tz":-120,"elapsed":211,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}}},"outputs":[],"source":["class SNLI():\n","    def __init__(self, batch_size, device):\n","        self.inputs = data.Field(lower=True, tokenize = None, batch_first = True)\n","        self.labels = data.Field(sequential = False, unk_token = None, is_target = True)\n","        self.train, self.val, self.test = datasets.SNLI.splits(self.inputs, self.labels)\n","        self.inputs.build_vocab(self.train, self.val)\n","        self.labels.build_vocab(self.train)\n","        self.train_iter, self.val_iter, self.test_iter = data.Iterator.splits((self.train, self.val, self.test), batch_size = batch_size, device=device)\n","    \n","    def vocabulary_size(self):\n","        return len(self.inputs.vocab)\n","    \n","    def out_dim(self):\n","        return len(self.labels.vocab)\n","    \n","    def labels(self):\n","        return self.labels.vocab.stoi"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4RSZeuxr5j33","executionInfo":{"status":"ok","timestamp":1651497391489,"user_tz":-120,"elapsed":199,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}}},"outputs":[],"source":["class BiLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, dropout_ratio, hidden_dim, out_dim, bidirect):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.projection = nn.Linear(embedding_dim, 300)\n","        self.dropout = nn.Dropout(p = dropout_ratio)\n","        self.lstm = nn.LSTM(300, hidden_dim, 3, bidirectional=bidirect)\n","        self.relu = nn.ReLU()\n","        self.out = nn.Sequential(\n","            nn.Linear(2*512, 512),\n","\t\t\tself.relu,\n","\t\t\tself.dropout,\n","\t\t\tnn.Linear(512, 256),\n","\t\t\tself.relu,\n","\t\t\tself.dropout,\n","\t\t\tnn.Linear(256, 256),\n","\t\t\tself.relu,\n","\t\t\tself.dropout,\n","\t\t\tnn.Linear(256, out_dim)\n","        )\n","        pass\n","    def forward(self, batch):\n","        premise_embedding = self.embedding(batch.premise)\n","        hypothesis_embedding = self.embedding(batch.hypothesis)\n","        premise_projection = self.relu(self.projection(premise_embedding))\n","        hypothesis_projection = self.relu(self.projection(hypothesis_embedding))\n","        encoded_premise, _ = self.lstm(premise_projection)\n","        encoded_hypothesis, _ = self.lstm(hypothesis_projection)\n","        premise = encoded_premise.sum(dim=1)\n","        hypothesis = encoded_hypothesis.sum(dim=1)\n","        combined = torch.cat((premise, hypothesis), 1)\n","        return self.out(combined)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FiQ22omY5j33","executionInfo":{"status":"ok","timestamp":1651497393807,"user_tz":-120,"elapsed":270,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}}},"outputs":[],"source":["def get_optimizer(model, opt_name, lr, l2_penalty, momentum=None):\n","        if opt_name == 'SGD':\n","            return optim.SGD(\n","                model.parameters(), lr, weight_decay=l2_penalty)\n","        elif opt_name == 'Momentum':\n","            return optim.SGD(\n","                model.parameters(), lr=lr, momentum=momentum,\n","                weight_decay=l2_penalty)\n","        elif opt_name == 'Nesterov':\n","            return optim.SGD(\n","                model.parameters(), lr=lr, momentum=momentum,\n","                weight_decay=l2_penalty, nesterov=True)\n","        elif opt_name == 'Adagrad':\n","            return optim.Adagrad(\n","                model.parameters(), lr=lr, weight_decay=l2_penalty)\n","        elif opt_name == 'RMSProp':\n","            return optim.RMSprop(\n","                model.parameters(), lr=lr, weight_decay=l2_penalty)\n","        elif opt_name == 'Adam':\n","            return optim.Adam(\n","                model.parameters(), lr=lr, weight_decay=l2_penalty)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"p8h60IvC5j34","executionInfo":{"status":"ok","timestamp":1651497395392,"user_tz":-120,"elapsed":216,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}}},"outputs":[],"source":["def train(model, dataset):\n","    model.train()\n","    dataset.train_iter.init_epoch()\n","    correct = 0\n","    total = 0\n","    n_loss = 0\n","    for batch_idx, batch in enumerate(dataset.train_iter):\n","        model.optimizer.zero_grad()\n","        prediction = model(batch)\n","        loss = F.cross_entropy(prediction, batch.label)\n","        correct += (torch.max(prediction, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n","        total += batch.batch_size\n","        n_loss += loss.item()\n","        loss.backward()\n","        model.optimizer.step()\n","    \n","    train_loss = n_loss/total;\n","    train_acc = (correct/total) * 100.\n","    return train_loss, train_acc"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"GJ-LZKIM5j34","executionInfo":{"status":"ok","timestamp":1651497396977,"user_tz":-120,"elapsed":315,"user":{"displayName":"pierric meleard","userId":"17135687950067136623"}}},"outputs":[],"source":["def validate(model, dataset):\n","    model.eval()\n","    dataset.val_iter.init_epoch()\n","    correct = 0\n","    total = 0\n","    n_loss = 0\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(dataset.val_iter):\n","            prediction = model(batch)\n","            loss = F.cross_entropy(prediction, batch.label)\n","            correct += (torch.max(prediction, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n","            total += batch.batch_size\n","            n_loss += loss.item()\n","    \n","    val_loss = n_loss/total;\n","    val_acc = (correct/total) * 100.\n","    return val_loss, val_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEtEJM2b5j35","outputId":"b69a7eb9-50cb-4fb9-88e8-7fbc66a6e7c6"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["cpu\n","downloading snli_1.0.zip\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["snli_1.0.zip: 100%|██████████| 94.6M/94.6M [00:03<00:00, 25.4MB/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["extracting\n"]}],"source":["if __name__ == \"__main__\":\n","    batch_size = 512\n","    embedding_dim = 150\n","    dropout_ratio = 0.2\n","    hidden_dim = 256\n","    epochs = 10\n","    lr = 0.001\n","    bidirect = True\n","    combine = 'cat'\n","    if torch.cuda.is_available():\n","            torch.cuda.set_device(0)\n","            device = torch.device('cuda:{}'.format(0))\n","    else:\n","        device = torch.device('cpu')\n","    print(device)\n","    opt_name = 'Adam'\n","    l2_penalty = 0\n","    momentum = None\n","    dataset = SNLI(batch_size, device)\n","    out_dim = dataset.out_dim()\n","    vocab_size = dataset.vocabulary_size()\n","    model = BiLSTM(vocab_size, embedding_dim, dropout_ratio, hidden_dim, out_dim, bidirect)\n","    model.to(device)\n","    model.optimizer = get_optimizer(model, opt_name, lr, l2_penalty)\n","    \n","    vocab_size = dataset.vocabulary_size()\n","    train_loss = []\n","    train_acc = []\n","    val_loss = []\n","    val_acc = []\n","    for epoch in range(epochs):\n","        start = time.time()\n","        training_loss, training_accuracy = train(model, dataset)\n","        validation_loss, validation_accuracy = validate(model, dataset)\n","        stop = time.time()\n","        train_loss.append(training_loss)\n","        train_acc.append(training_accuracy)\n","        val_loss.append(validation_loss)\n","        val_acc.append(validation_accuracy)\n","        print(\"Time: {}, Epoch: {}, Training loss: {}, Training Accuracy: {}, Validation loss: {}, Validation Accuracy: {}\".format(stop-start, epoch+1, training_loss, training_accuracy, validation_loss, validation_accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dZzNix15j35"},"outputs":[],"source":["    from google.colab import drive\n","    drive.mount('/content/drive')\n","    filename = \"/content/drive/MyDrive/\" + '{0}_{1}_{2}_{3}_{4}_{5}_{6}_bidirect.pt'.format(batch_size, embedding_dim, dropout_ratio, hidden_dim, epochs, opt_name, lr)\n","    torch.save(model.state_dict(), filename)\n","    print(filename + ' saved successfully\\n')\n","    fig = plt.figure()\n","    plt.plot(train_loss, label = 'Training Loss')\n","    plt.plot(val_loss, label = 'Validation Loss')\n","    plt.title(\"Loss LSTM\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    filename = \"/content/drive/MyDrive/Plots/\" + '{0}_{1}_{2}_{3}_{4}_{5}_{6}_bidirect.png'.format(batch_size, embedding_dim, dropout_ratio, hidden_dim, epochs, opt_name, lr)\n","    plt.savefig(filename)\n","    plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"bilstm.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}